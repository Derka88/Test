{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([900, 100], dtype=int64))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Étape 1 : Création d'un dataset déséquilibré pour une tâche de classification binaire.\n",
    "# Nous utilisons make_classification pour générer des données artificielles avec une classe majoritaire (90%) \n",
    "# et une classe minoritaire (10%).\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=8, \n",
    "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
    "\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([619, 619], dtype=int64))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Étape 2 : Traitement du déséquilibre des classes avec SMOTETomek.\n",
    "# SMOTE est utilisé pour suréchantillonner la classe minoritaire, tandis que Tomek Links nettoie les points ambigus.\n",
    "# Le résultat est un dataset d'entraînement équilibré, prêt pour l'entraînement du modèle.\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smt = SMOTETomek(random_state=42)\n",
    "X_train_res, y_train_res = smt.fit_resample(X_train, y_train)\n",
    "np.unique(y_train_res, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des modèles avec leurs paramètres et les jeux de données associés.\n",
    "# - Logistic Regression : Données d'origine.\n",
    "# - Random Forest : Données d'origine.\n",
    "# - XGBoost : Données d'origine.\n",
    "# - XGBoost avec SMOTE : Données équilibrées pour l'entraînement.\n",
    "models = [\n",
    "    (\n",
    "        \"Logistic Regression\", \n",
    "        {\"C\": 1, \"solver\": 'liblinear'},\n",
    "        LogisticRegression(), \n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest\", \n",
    "        {\"n_estimators\": 30, \"max_depth\": 3},\n",
    "        RandomForestClassifier(), \n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"XGBClassifier\",\n",
    "        {\"use_label_encoder\": False, \"eval_metric\": 'logloss'},\n",
    "        XGBClassifier(), \n",
    "        (X_train, y_train),\n",
    "        (X_test, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"XGBClassifier With SMOTE\",\n",
    "        {\"use_label_encoder\": False, \"eval_metric\": 'logloss'},\n",
    "        XGBClassifier(), \n",
    "        (X_train_res, y_train_res),\n",
    "        (X_test, y_test)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle pour entraîner et évaluer chaque modèle de la liste 'models'.\n",
    "# Pour chaque modèle :\n",
    "# 1. Configuration des hyperparamètres avec set_params.\n",
    "# 2. Entraînement sur les données d'entraînement.\n",
    "# 3. Prédiction sur les données de test.\n",
    "# 4. Calcul du rapport de classification (precision, recall, f1-score) avec classification_report.\n",
    "# 5. Sauvegarde du rapport dans une liste pour analyse ultérieure.\n",
    "\n",
    "reports = []\n",
    "\n",
    "for model_name, params, model, train_set, test_set in models:\n",
    "    X_train = train_set[0]\n",
    "    y_train = train_set[1]\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "    \n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    reports.append(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des bibliothèques MLflow et DagsHub pour le suivi des expérimentations.\n",
    "mlflow.sklearn et mlflow.xgboost ajoutent le support des modèles scikit-learn et XGBoost.\n",
    "dagshub.init permet d'intégrer MLflow avec un repository DagsHub pour un suivi centralisé des expérimentations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='kadermamoudou88', repo_name='Test', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des variables d'environnement pour l'authentification avec le serveur MLflow hébergé sur DagsHub.\n",
    "# MLFLOW_TRACKING_USERNAME : Nom d'utilisateur pour DagsHub.\n",
    "# MLFLOW_TRACKING_PASSWORD : Token d'accès pour authentification sécurisée.\n",
    "# MLFLOW_TRACKING_URI : URI du serveur distant MLflow pour le suivi des expérimentations.\n",
    "import os \n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'kadermamoudou88'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = 'cdc80ad9f32ee76bc2993288b2b6402b2b88f646'\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'https://dagshub.com/kadermamoudou88/Test.mlflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et enregistrement des expérimentations avec MLflow\n",
    "mlflow.set_experiment(\"Anomaly Detection\")  # Définition du nom de l'expérience\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/kadermamoudou88/Test.mlflow\")  # Configuration du serveur distant\n",
    "\n",
    "# Boucle pour journaliser chaque modèle et ses performances dans MLflow\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]  # Nom du modèle\n",
    "    params = element[1]  # Hyperparamètres\n",
    "    model = element[2]  # Instance du modèle\n",
    "    report = reports[i]  # Rapport de classification\n",
    "    \n",
    "    # Démarrage d'un run MLflow\n",
    "    with mlflow.start_run(run_name=model_name):  \n",
    "        mlflow.log_params(params)  # Journalisation des hyperparamètres\n",
    "        mlflow.log_metrics({       # Journalisation des métriques de performance\n",
    "            'accuracy': report['accuracy'],\n",
    "            'recall_class_1': report['1']['recall'],\n",
    "            'recall_class_0': report['0']['recall'],\n",
    "            'f1_score_macro': report['macro avg']['f1-score']\n",
    "        })  \n",
    "        \n",
    "        # Enregistrement du modèle entraîné\n",
    "        if \"XGB\" in model_name:\n",
    "            mlflow.xgboost.log_model(model, \"model\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
